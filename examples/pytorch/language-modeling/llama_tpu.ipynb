{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOYpGLfe9jv16vOjU0Cz1ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinKimata/transformers/blob/main/examples/pytorch/language-modeling/llama_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDzB-vE9vp_o",
        "outputId": "56eff703-13db-48e6-b49a-841bcb08219b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:           334Gi       2.0Gi       316Gi       2.0Mi        16Gi       330Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch~=2.4.0 torch_xla[tpu]~=2.4.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "!git clone -b llama2-google-next-training https://github.com/pytorch-tpu/transformers.git\n",
        "%cd transformers\n",
        "!pip install peft\n",
        "!pip install -e . --user\n",
        "!pip install datasets accelerate evaluate scikit-learn\n",
        "!pip uninstall -y tensorflow jax"
      ],
      "metadata": {
        "id": "7rhBuyp8vtUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AlvinKimata/transformers/main/examples/pytorch/language-modeling/run_clm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3yuABWbdHsQ",
        "outputId": "c81297f8-d67a-42ad-a19e-4bef6bfbf1ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-21 10:21:47--  https://raw.githubusercontent.com/AlvinKimata/transformers/main/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37868 (37K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  36.98K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-08-21 10:21:47 (3.44 MB/s) - ‘run_clm.py’ saved [37868/37868]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhMuqVWt4FWJ",
        "outputId": "a5699210-3ec1-4d0b-c323-b3d2878af35b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "!python run_clm.py \\\n",
        "  --tokenizer_name hf-internal-testing/llama-tokenizer \\\n",
        "  --dataset_name wikitext \\\n",
        "  --dataset_config_name wikitext-2-raw-v1 \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --num_train_epochs 2 \\\n",
        "  --do_train \\\n",
        "  --output_dir /tmp/output \\\n",
        "  --overwrite_output_dir \\\n",
        "  --model_name_or_path \"meta-llama/Meta-Llama-3-8B\"\\\n",
        "  --save_strategy no \\\n",
        "  --logging_strategy no \\\n",
        "  --remove_unused_columns no \\\n",
        "  --optim adafactor \\\n",
        "  --torch_dtype bfloat16 \\\n",
        "  --dataloader_drop_last yes \\\n",
        "  --block_size 256 \\\n",
        "  --spmd_2d_sharding 0 \\\n",
        "  --spmd_grad_chkpt True \\\n",
        "  --spmd_fsdp_sharding True \\\n",
        "  --peft_lora True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qRUxRthzP9_",
        "outputId": "fd34ab97-88b8-496e-f34d-95b468d974df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: 0, device: xla:0, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "checkpoint_manager_path=None,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/output/runs/Aug21_10-48-02_046768babffb,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=no,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adafactor,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/tmp/output,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/output,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=no,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla_autocast=False,\n",
            "xla_cache_path=None,\n",
            "xla_cache_single_writer=False,\n",
            "xla_execution_time_step=None,\n",
            "xla_measure_avg_step_time=False,\n",
            ")\n",
            "INFO:__main__:Profiling server started: <_XLAC.profiler.ProfilerServer object at 0x7a9d702738b0>\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
            "INFO:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-08-21 10:48:20,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-08-21 10:48:20,168 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-08-21 10:48:20,243 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-08-21 10:48:20,243 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-08-21 10:48:20,243 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-08-21 10:48:20,243 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-08-21 10:48:20,243 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer_config.json\n",
            "Number of devices: 8\n",
            "Model axis is: 2\n",
            "[INFO|modeling_utils.py:3429] 2024-08-21 10:48:20,360 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1494] 2024-08-21 10:48:20,361 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:928] 2024-08-21 10:48:20,362 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:02<00:00,  1.50it/s]\n",
            "[INFO|modeling_utils.py:4170] 2024-08-21 10:48:23,180 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-08-21 10:48:23,180 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:883] 2024-08-21 10:48:23,259 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-08-21 10:48:23,260 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "> [FSDP] Sharding tensor model.embed_tokens.weight torch.Size([128256, 4096]) torch.bfloat16\n",
            "model.embed_tokens.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.0.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.0.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.0.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.0.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.0.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.0.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.0.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.0.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.1.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.1.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.1.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.1.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.1.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.1.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.1.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.1.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.2.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.2.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.2.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.2.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.2.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.2.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.2.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.2.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.3.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.3.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.3.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.3.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.3.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.3.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.3.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.3.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.4.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.4.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.4.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.4.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.4.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.4.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.4.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.4.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.5.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.5.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.5.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.5.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.5.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.5.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.5.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.5.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.6.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.6.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.6.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.6.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.6.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.6.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.6.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.6.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.7.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.7.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.7.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.7.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.7.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.7.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.7.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.7.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.8.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.8.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.8.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.8.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.8.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.8.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.8.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.8.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.9.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.9.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.9.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.9.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.9.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.9.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.9.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.9.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.10.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.10.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.10.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.10.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.10.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.10.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.10.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.10.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.11.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.11.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.11.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.11.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.11.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.11.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.11.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.11.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.12.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.12.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.12.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.12.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.12.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.12.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.12.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.12.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.13.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.13.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.13.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.13.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.13.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.13.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.13.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.13.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.14.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.14.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.14.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.14.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.14.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.14.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.14.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.14.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.15.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.15.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.15.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.15.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.15.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.15.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.15.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.15.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.16.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.16.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.16.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.16.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.16.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.16.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.16.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.16.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.17.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.17.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.17.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.17.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.17.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.17.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.17.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.17.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.18.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.18.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.18.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.18.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.18.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.18.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.18.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.18.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.19.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.19.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.19.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.19.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.19.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.19.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.19.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.19.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.20.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.20.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.20.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.20.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.20.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.20.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.20.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.20.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.21.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.21.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.21.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.21.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.21.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.21.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.21.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.21.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.22.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.22.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.22.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.22.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.22.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.22.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.22.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.22.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.23.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.23.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.23.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.23.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.23.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.23.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.23.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.23.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.24.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.24.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.24.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.24.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.24.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.24.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.24.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.24.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.25.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.25.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.25.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.25.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.25.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.25.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.25.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.25.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.26.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.26.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.26.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.26.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.26.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.26.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.26.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.26.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.27.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.27.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.27.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.27.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.27.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.27.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.27.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.27.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.28.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.28.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.28.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.28.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.28.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.28.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.28.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.28.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.29.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.29.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.29.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.29.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.29.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.29.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.29.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.29.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.30.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.30.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.30.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.30.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.30.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.30.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.30.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.30.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.31.self_attn.q_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.31.self_attn.k_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096]) torch.bfloat16\n",
            "model.layers.31.self_attn.v_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.bfloat16\n",
            "model.layers.31.self_attn.o_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.31.mlp.gate_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096]) torch.bfloat16\n",
            "model.layers.31.mlp.up_proj.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336]) torch.bfloat16\n",
            "model.layers.31.mlp.down_proj.weight {devices=[1,4,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "> [FSDP] Sharding tensor model.layers.31.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor model.norm.weight torch.Size([4096]) torch.bfloat16\n",
            "> [FSDP] Sharding tensor lm_head.weight torch.Size([128256, 4096]) torch.bfloat16\n",
            "lm_head.weight {devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}\n",
            "WARNING:__main__:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "Applying gradient checkpointing\n",
            "LoRA enabled\n",
            "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e601b5ad671fdbf7.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e601b5ad671fdbf7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1b6980b00ea08f9e.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-1b6980b00ea08f9e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d9d836bbd62e5ab8.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-d9d836bbd62e5ab8.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-51af3910fd1a5355.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-51af3910fd1a5355.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a124ac06c62db779.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-a124ac06c62db779.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2293682540e3cd41.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-2293682540e3cd41.arrow\n",
            "[INFO|trainer.py:2091] 2024-08-21 10:48:31,468 >> ***** Running training *****\n",
            "[INFO|trainer.py:2092] 2024-08-21 10:48:31,468 >>   Num examples = 11,158\n",
            "[INFO|trainer.py:2093] 2024-08-21 10:48:31,468 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:2094] 2024-08-21 10:48:31,468 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:2097] 2024-08-21 10:48:31,468 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2098] 2024-08-21 10:48:31,468 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2099] 2024-08-21 10:48:31,468 >>   Total optimization steps = 2,788\n",
            "[INFO|trainer.py:2100] 2024-08-21 10:48:31,470 >>   Number of trainable parameters = 3,407,872\n",
            "  0% 0/2788 [00:00<?, ?it/s]input sharding {'input_ids': (torch.Size([8, 256]), '{devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}'), 'attention_mask': (torch.Size([8, 256]), '{devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}'), 'labels': (torch.Size([8, 256]), '{devices=[4,1,2]0,1,2,3,4,5,6,7 last_tile_dim_replicate}')}\n",
            "/usr/local/lib/python3.10/dist-packages/torch_xla/utils/checkpoint.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
            "/usr/local/lib/python3.10/dist-packages/torch_xla/utils/checkpoint.py:172: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n",
            " 37% 1029/2788 [28:25<45:40,  1.56s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DhxdpEG7bWi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}