{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDzB-vE9vp_o",
        "outputId": "338390ce-a9a2-4ea7-8b8f-a4014ce765a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:           334Gi       2.1Gi       316Gi       2.0Mi        16Gi       330Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers==4.43.1 wandb"
      ],
      "metadata": {
        "id": "ANKKHT4TYb-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmEMRAf6Y1Lv",
        "outputId": "656e71b4-ce69-416e-b15a-21a145747f36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.18.0+cpu\n",
            "Uninstalling torchvision-0.18.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.18.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7rhBuyp8vtUb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch~=2.4.0 torch_xla[tpu]~=2.4.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "# !git clone -b llama2-google-next-training https://github.com/pytorch-tpu/transformers.git\n",
        "%cd transformers\n",
        "!pip install peft\n",
        "!pip install -e . --user\n",
        "!pip install datasets accelerate evaluate scikit-learn\n",
        "!pip uninstall -y tensorflow jax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AlvinKimata/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0HHTmhLTphT",
        "outputId": "5fae3cb7-3d22-4bf3-a748-f6e5f1ac41bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 187217, done.\u001b[K\n",
            "remote: Counting objects: 100% (22845/22845), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1815/1815), done.\u001b[K\n",
            "remote: Total 187217 (delta 22243), reused 21067 (delta 21018), pack-reused 164372 (from 1)\u001b[K\n",
            "Receiving objects: 100% (187217/187217), 206.40 MiB | 35.16 MiB/s, done.\n",
            "Resolving deltas: 100% (134990/134990), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txAxrzwIZhyS",
        "outputId": "ecceb3a5-712a-46d3-ba85-92d3375e92cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhMuqVWt4FWJ",
        "outputId": "972cc778-e310-4565-a664-da41989a3794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3yuABWbdHsQ",
        "outputId": "13c7f6a3-beb0-40aa-fb14-512224fdf83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers/transformers/examples/pytorch/language-modeling\n"
          ]
        }
      ],
      "source": [
        "%cd transformers/examples/pytorch/language-modeling/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key = \"\", verify = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVoieiU9sBDf",
        "outputId": "381288f9-a272-4431-9504-149acb0fe24b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export WANDB_PROJECT=\"law-llama-instruct\""
      ],
      "metadata": {
        "id": "SiZWte1xsmNG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qRUxRthzP9_",
        "outputId": "26bbf83c-93fa-4366-b1ca-416be62f2db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:2007: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:2026: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case Kimata/law-llama-instruct).\n",
            "  warnings.warn(\n",
            "WARNING:__main__:Process rank: 0, device: xla:0, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=100,\n",
            "eval_strategy=steps,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=Kimata/law-llama-instruct,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/output/runs/Aug26_17-29-21_e7b57beda44b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adafactor,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/tmp/output,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=law-llama-instruct,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/output,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "INFO:__main__:Profiling server started: <_XLAC.profiler.ProfilerServer object at 0x7ebabadabf30>\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "INFO:datasets.builder:Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "INFO:datasets.builder:Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/CognifuseRL--kenyan_law_dataset/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "INFO:datasets.builder:Found cached dataset kenyan_law_dataset (/root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3\n",
            "[INFO|configuration_utils.py:733] 2024-08-26 17:29:29,701 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-08-26 17:29:29,702 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.43.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-26 17:29:29,755 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-26 17:29:29,755 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-26 17:29:29,755 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-26 17:29:29,755 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2289] 2024-08-26 17:29:29,755 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--hf-internal-testing--llama-tokenizer/snapshots/d02ad6cb9dd2c2296a6332199fa2fdca5938fef0/tokenizer_config.json\n",
            "[WARNING|logging.py:328] 2024-08-26 17:29:29,797 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
            "Number of devices: 8\n",
            "Model axis is: 2\n",
            "[INFO|modeling_utils.py:3621] 2024-08-26 17:29:29,866 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1569] 2024-08-26 17:29:29,867 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-08-26 17:29:29,868 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00,  5.48it/s]\n",
            "[INFO|modeling_utils.py:4450] 2024-08-26 17:29:30,726 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4458] 2024-08-26 17:29:30,726 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:993] 2024-08-26 17:29:30,779 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-08-26 17:29:30,779 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "> [2D] Sharding tensor model.embed_tokens.weight torch.Size([128256, 4096])\n",
            "model.embed_tokens.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.0.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.0.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.0.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.0.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.0.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.0.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.0.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.0.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.1.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.1.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.1.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.1.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.1.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.1.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.1.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.1.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.2.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.2.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.2.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.2.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.2.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.2.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.2.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.2.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.3.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.3.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.3.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.3.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.3.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.3.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.3.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.3.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.4.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.4.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.4.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.4.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.4.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.4.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.4.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.4.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.5.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.5.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.5.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.5.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.5.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.5.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.5.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.5.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.6.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.6.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.6.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.6.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.6.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.6.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.6.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.6.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.7.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.7.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.7.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.7.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.7.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.7.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.7.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.7.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.8.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.8.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.8.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.8.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.8.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.8.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.8.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.8.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.9.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.9.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.9.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.9.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.9.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.9.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.9.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.9.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.10.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.10.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.10.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.10.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.10.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.10.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.10.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.10.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.11.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.11.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.11.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.11.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.11.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.11.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.11.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.11.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.12.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.12.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.12.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.12.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.12.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.12.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.12.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.12.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.13.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.13.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.13.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.13.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.13.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.13.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.13.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.13.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.14.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.14.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.14.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.14.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.14.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.14.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.14.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.14.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.15.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.15.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.15.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.15.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.15.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.15.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.15.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.15.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.16.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.16.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.16.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.16.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.16.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.16.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.16.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.16.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.17.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.17.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.17.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.17.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.17.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.17.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.17.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.17.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.18.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.18.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.18.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.18.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.18.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.18.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.18.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.18.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.19.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.19.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.19.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.19.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.19.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.19.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.19.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.19.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.20.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.20.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.20.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.20.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.20.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.20.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.20.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.20.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.21.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.21.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.21.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.21.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.21.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.21.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.21.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.21.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.22.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.22.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.22.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.22.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.22.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.22.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.22.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.22.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.23.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.23.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.23.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.23.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.23.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.23.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.23.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.23.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.24.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.24.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.24.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.24.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.24.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.24.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.24.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.24.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.25.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.25.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.25.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.25.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.25.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.25.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.25.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.25.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.26.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.26.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.26.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.26.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.26.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.26.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.26.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.26.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.27.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.27.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.27.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.27.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.27.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.27.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.27.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.27.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.28.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.28.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.28.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.28.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.28.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.28.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.28.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.28.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.29.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.29.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.29.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.29.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.29.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.29.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.29.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.29.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.30.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.30.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.30.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.30.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.30.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.30.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.30.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.30.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.31.self_attn.q_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.31.self_attn.k_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
            "model.layers.31.self_attn.v_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
            "model.layers.31.self_attn.o_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.31.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
            "model.layers.31.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "> [2D] Sharding tensor model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
            "model.layers.31.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
            "> [2D] Sharding tensor model.layers.31.input_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor model.norm.weight torch.Size([4096])\n",
            "> [2D] Sharding tensor lm_head.weight torch.Size([128256, 4096])\n",
            "lm_head.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
            "WARNING:__main__:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "Applying gradient checkpointing\n",
            "LoRA enabled\n",
            "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-1ae23d95cd72f5e2.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-1ae23d95cd72f5e2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-3a5b00fcb14be1b9.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-3a5b00fcb14be1b9.arrow\n",
            "WARNING:__main__:The block_size passed (4096) is larger than the maximum length for the model (2048). Using block_size=2048.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-cc143be404c155a2.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-cc143be404c155a2.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-adcfd734236f124e.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/CognifuseRL___kenyan_law_dataset/default/0.0.0/da3589a1399badb5e02d3f725327184bb61e4113385c9aaea58803a8909783e3/cache-adcfd734236f124e.arrow\n",
            "[INFO|trainer.py:2134] 2024-08-26 17:29:37,224 >> ***** Running training *****\n",
            "[INFO|trainer.py:2135] 2024-08-26 17:29:37,224 >>   Num examples = 10,198\n",
            "[INFO|trainer.py:2136] 2024-08-26 17:29:37,224 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2137] 2024-08-26 17:29:37,224 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2140] 2024-08-26 17:29:37,224 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:2141] 2024-08-26 17:29:37,224 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2142] 2024-08-26 17:29:37,224 >>   Total optimization steps = 5,099\n",
            "[INFO|trainer.py:2143] 2024-08-26 17:29:37,226 >>   Number of trainable parameters = 3,407,872\n",
            "[INFO|integration_utils.py:786] 2024-08-26 17:29:37,228 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimata\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/transformers/transformers/examples/pytorch/language-modeling/wandb/run-20240826_172937-fc70dyyu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/tmp/output\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kimata/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kimata/huggingface/runs/fc70dyyu\u001b[0m\n",
            "  0% 0/5099 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch_xla/utils/checkpoint.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
            "/usr/local/lib/python3.10/dist-packages/torch_xla/utils/checkpoint.py:172: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n",
            "  1% 48/5099 [05:05<5:00:36,  3.57s/it]"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "!python run_clm.py \\\n",
        "  --tokenizer_name hf-internal-testing/llama-tokenizer \\\n",
        "  --dataset_name CognifuseRL/kenyan_law_dataset \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --per_device_eval_batch_size 2 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --evaluation_strategy \"steps\" \\\n",
        "  --eval_steps 100 \\\n",
        "  --output_dir /tmp/output \\\n",
        "  --overwrite_output_dir \\\n",
        "  --model_name_or_path \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\\\n",
        "  --save_strategy \"epoch\" \\\n",
        "  --logging_strategy \"steps\" \\\n",
        "  --remove_unused_columns no \\\n",
        "  --optim adafactor \\\n",
        "  --torch_dtype bfloat16 \\\n",
        "  --dataloader_drop_last yes \\\n",
        "  --block_size 4096 \\\n",
        "  --spmd_2d_sharding 1 \\\n",
        "  --spmd_debug True \\\n",
        "  --spmd_grad_chkpt True \\\n",
        "  --peft_lora True \\\n",
        "  --trust_remote_code True \\\n",
        "  --push_to_hub True \\\n",
        "  --push_to_hub_model_id \"law-llama-instruct\" \\\n",
        "  --push_to_hub_token \"\"\\\n",
        "  --report_to 'wandb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8DhxdpEG7bWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e372ca-8dc6-470b-9805-52e9554858eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  events.out.tfevents.1724689968.e7b57beda44b.31525.0\n"
          ]
        }
      ],
      "source": [
        "!ls -a /tmp/output/runs/Aug26_16-32-31_e7b57beda44b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -a /tmp/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_QY4x-OpZvZ",
        "outputId": "2c84482b-db4a-474a-d326-8541519faa9f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[30;42m..\u001b[0m/  \u001b[01;34mruns\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/run.zip /tmp/output/runs/Aug26_16-32-31_e7b57beda44b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IJO8455p-9n",
        "outputId": "0d680eb7-096f-4309-b1e6-2f92282cdbcf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: tmp/output/runs/Aug26_16-32-31_e7b57beda44b/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /tmp/output/runs/Aug26_16-32-31_e7b57beda44b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03kFBplGqDJN",
        "outputId": "866f2280-ed1c-4fb1-cefe-6216bc2644ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12K\t/tmp/output/runs/Aug26_16-32-31_e7b57beda44b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R16enudQqLFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
